{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9fc1850",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\MN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import google.generativeai as genai\n",
    "import chromadb\n",
    "from IPython.display import Markdown\n",
    "\n",
    "# Step 1: Extract text from the PDF\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    for page in doc:\n",
    "        text += page.get_text()\n",
    "    return text\n",
    "\n",
    "pdf_text = extract_text_from_pdf('dataset/best 55 places.pdf')\n",
    "\n",
    "# Step 2: Improved text splitting\n",
    "def split_text(text, max_chunk_size=1000):\n",
    "    sentences = sent_tokenize(text)\n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        if len(current_chunk) + len(sentence) + 1 <= max_chunk_size:\n",
    "            current_chunk += \" \" + sentence\n",
    "        else:\n",
    "            chunks.append(current_chunk.strip())\n",
    "            current_chunk = sentence\n",
    "    \n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk.strip())\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "text_chunks = split_text(pdf_text)\n",
    "\n",
    "# Step 3: Data Cleaning and Validation\n",
    "def clean_text(text):\n",
    "    clean_text = text.replace(\"\\n\", \" \").replace(\"\\r\", \" \").strip()\n",
    "    return clean_text\n",
    "\n",
    "cleaned_chunks = [clean_text(chunk) for chunk in text_chunks]\n",
    "\n",
    "# Step 4: Configure Google Gemini API\n",
    "API_KEY = 'AIzaSyDleZ4xVF9dCT7aw95WBeDpfHwktn4LUQ0'  # Replace with your actual API key\n",
    "genai.configure(api_key=API_KEY)\n",
    "\n",
    "class GeminiEmbeddingFunction:\n",
    "    def __call__(self, input):\n",
    "        model = 'models/text-embedding-004'\n",
    "        response = genai.embed_content(model=model, content=input, task_type=\"retrieval_document\")\n",
    "        if 'embedding' in response:\n",
    "            return response['embedding']\n",
    "        else:\n",
    "            raise KeyError(f\"'embedding' key not found in response: {response}\")\n",
    "\n",
    "# Step 5: Store embeddings in ChromaDB\n",
    "def create_chroma_db(documents, name):\n",
    "    chroma_client = chromadb.Client()\n",
    "\n",
    "    # Check if the collection exists\n",
    "    try:\n",
    "        existing_collection = chroma_client.get_collection(name=name)\n",
    "        chroma_client.delete_collection(name=name)\n",
    "    except ValueError:\n",
    "        pass  # Collection does not exist\n",
    "\n",
    "    db = chroma_client.create_collection(name=name, embedding_function=GeminiEmbeddingFunction())\n",
    "\n",
    "    for i, d in enumerate(documents):\n",
    "        db.add(\n",
    "            documents=[d],\n",
    "            ids=[str(i)]\n",
    "        )\n",
    "    return db\n",
    "\n",
    "db = create_chroma_db(cleaned_chunks, \"egypt_places_chromadb\")\n",
    "\n",
    "# Step 6: Prompt Refinement\n",
    "def test_prompt(prompt, function, *args):\n",
    "    try:\n",
    "        response = function(*args)\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        print(f\"Error testing prompt: {e}\")\n",
    "        return None\n",
    "\n",
    "# Example function to simulate embedding generation\n",
    "def generate_embedding(text):\n",
    "    model = 'models/text-embedding-004'\n",
    "    response = genai.embed_content(model=model, content=text, task_type=\"retrieval_document\")\n",
    "    if 'embedding' in response:\n",
    "        return response['embedding']\n",
    "    else:\n",
    "        raise KeyError(f\"'embedding' key not found in response: {response}\")\n",
    "\n",
    "# Querying and response generation functions\n",
    "def get_relevant_passage(query, db):\n",
    "    result = db.query(query_texts=[query], n_results=1)\n",
    "    passage = result['documents'][0][0]\n",
    "    return passage\n",
    "\n",
    "# Continuous Testing and Refinement\n",
    "def continuous_testing_and_refinement(db):\n",
    "    # Define initial prompts\n",
    "    prompts = {\n",
    "        \"embedding\": \"Generate an embedding for the following text: {text}\",\n",
    "        \"querying\": \"Find the most relevant information about {query}.\",\n",
    "        \"response\": \"Provide a detailed response for the query: {query}.\"\n",
    "    }\n",
    "\n",
    "    # Test and refine prompts iteratively\n",
    "    for prompt_type, prompt in prompts.items():\n",
    "        print(f\"Testing {prompt_type} prompt...\")\n",
    "        if prompt_type == \"embedding\":\n",
    "            text = \"Pyramids in Egypt\"\n",
    "            result = test_prompt(prompt, generate_embedding, text)\n",
    "        else:\n",
    "            query = \"Pyramids in Egypt\"\n",
    "            result = test_prompt(prompt, get_relevant_passage, query, db)\n",
    "        \n",
    "        if not result:\n",
    "            # Refine prompt if necessary\n",
    "            refined_prompt = f\"Refined {prompt_type} prompt text here: {query if prompt_type != 'embedding' else text}\"\n",
    "            result = test_prompt(refined_prompt, generate_embedding, text) if prompt_type == \"embedding\" else test_prompt(refined_prompt, get_relevant_passage, query, db)\n",
    "        \n",
    "        print(f\"Result for {prompt_type} prompt: {result}\")\n",
    "\n",
    "# Run continuous testing and refinement process\n",
    "continuous_testing_and_refinement(db)\n",
    "\n",
    "# Step 7: Query the Database\n",
    "query = \"Pyramids in Egypt\"\n",
    "passage = get_relevant_passage(query, db)\n",
    "Markdown(passage)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4011915",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
